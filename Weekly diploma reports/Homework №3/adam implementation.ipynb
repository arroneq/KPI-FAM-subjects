{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":414,"status":"ok","timestamp":1729192241558,"user":{"displayName":"Антон","userId":"12367554657538335492"},"user_tz":-180},"id":"U34O9zNe3FqP"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-10-19 16:47:35.211748: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n","2024-10-19 16:47:35.217020: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n","2024-10-19 16:47:35.234668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-10-19 16:47:35.264348: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-10-19 16:47:35.273207: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-19 16:47:35.292947: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-10-19 16:47:36.566720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import sys\n","realmin = sys.float_info.min\n","\n","import tensorflow as tf\n","import time\n","import numpy as np\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","\n","import time\n","\n","from scipy.optimize import minimize\n","from numba import jit, njit"]},{"cell_type":"markdown","metadata":{"id":"sAVh4m22nOz2"},"source":["### Adam Optimization Algorithm"]},{"cell_type":"markdown","metadata":{},"source":["#### Adam Tensorflow"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1729192266255,"user":{"displayName":"Антон","userId":"12367554657538335492"},"user_tz":-180},"id":"t8CYXBC_bX0r"},"outputs":[],"source":["def adam_tensorflow(fun, x0, tol):\n","    start_time = time.time()\n","\n","    x = tf.Variable(x0) # Define the variable x (initial guess) as a tensor\n","\n","    tolerance = tol # Set your desired tolerance value\n","\n","    learning_rate = 0.1\n","    # The learning rate. Defaults to `0.001`.\n","\n","    beta_1 = 0.9\n","    # A float value or a constant float tensor, or a callable that\n","    # takes no arguments and returns the actual value to use. The\n","    # exponential decay rate for the 1st moment estimates. Defaults to `0.9`.\n","\n","    beta_2 = 0.999\n","    # A float value or a constant float tensor, or a callable\n","    # that takes no arguments and returns the actual value to use. The\n","    # exponential decay rate for the 2nd moment estimates. Defaults to `0.999`.\n","\n","    epsilon = 1e-8\n","    # A small constant for numerical stability. This epsilon is \"epsilon hat\" in\n","    # the Kingma and Ba paper (in the formula just before Section 2.1),\n","    # not the epsilon in Algorithm 1 of the paper. Defaults to `1e-7`.\n","\n","    iteration_limit = 5000\n","\n","    optimizer = tf.optimizers.Adam(\n","        learning_rate, beta_1, beta_2, epsilon\n","    )\n","\n","    # Initialize a variable to store the previous value of the objective function\n","    prev_loss = float('inf')\n","\n","    iter = 0\n","    # Perform optimization with iteration limit and function tolerance\n","    while True:\n","        iter += 1\n","\n","        with tf.GradientTape() as tape:\n","            loss = fun(x) # Define the objective function\n","\n","        # Compute the gradient of the loss with respect to x\n","        grads = tape.gradient(loss, [x])\n","\n","        # Apply the gradients to update x\n","        optimizer.apply_gradients(zip(grads, [x]))\n","\n","        # Check if the difference in the loss is less than the tolerance\n","        loss_change = abs(prev_loss - loss.numpy())\n","        if loss_change < tolerance:\n","            break # Stop the optimization when tolerance is met\n","\n","        # Update the previous loss\n","        prev_loss = loss.numpy()\n","\n","        if iter >= iteration_limit:\n","            break # Stop the optimization when tolerance is met\n","\n","    end_time = time.time()\n","\n","    class solution:\n","        def __init__(self):\n","            self.x = x\n","            self.func = fun(x)\n","            self.iters = iter\n","            self.time = end_time - start_time\n","\n","    return solution()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1729192266256,"user":{"displayName":"Антон","userId":"12367554657538335492"},"user_tz":-180},"id":"j_w_vnS6c85y"},"outputs":[],"source":["def objective(x):\n","    return (x[0] - 7)**2 + (x[1] - 2)**2"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1390,"status":"ok","timestamp":1729192267632,"user":{"displayName":"Антон","userId":"12367554657538335492"},"user_tz":-180},"id":"YsffLwtOc6QW"},"outputs":[],"source":["result = adam_tensorflow(\n","    fun = objective,\n","    x0 = [0.0, 0.0],\n","    tol = 0.01\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([6.704236 , 1.9979095], dtype=float32)>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["result.x"]},{"cell_type":"markdown","metadata":{},"source":["#### Adam Njit"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1729192267636,"user":{"displayName":"Антон","userId":"12367554657538335492"},"user_tz":-180},"id":"oepw1MU1o9JL"},"outputs":[],"source":["@njit\n","def objective_function(x):\n","    return (x[0] - 7)**2 + (x[1] - 2)**2"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1729192267636,"user":{"displayName":"Антон","userId":"12367554657538335492"},"user_tz":-180},"id":"VI81f1vion9e"},"outputs":[],"source":["@njit\n","def adam_gradient(x):\n","    \"\"\"Numerical gradient approximation using central differences.\"\"\"\n","\n","    eps = 1e-8\n","    grad = np.zeros_like(x)\n","\n","    for i in range(len(x)):\n","        x_plus = x.copy()\n","        x_minus = x.copy()\n","        x_plus[i] += eps\n","        x_minus[i] -= eps\n","        grad[i] = (objective_function(x_plus) - objective_function(x_minus)) / (2 * eps)\n","\n","    return grad"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1729192267637,"user":{"displayName":"Антон","userId":"12367554657538335492"},"user_tz":-180},"id":"CLpz6G4coUaM"},"outputs":[],"source":["@njit\n","def adam_solver_numerical(x0, tol):\n","    x = x0.copy() # Initial guess\n","\n","    tolerance = tol # Set your desired tolerance value\n","\n","    learning_rate = 0.1\n","    # The learning rate. Defaults to `0.001`.\n","\n","    beta_1 = 0.9\n","    # A float value or a constant float tensor, or a callable that\n","    # takes no arguments and returns the actual value to use. The\n","    # exponential decay rate for the 1st moment estimates. Defaults to `0.9`.\n","\n","    beta_2 = 0.999\n","    # A float value or a constant float tensor, or a callable\n","    # that takes no arguments and returns the actual value to use. The\n","    # exponential decay rate for the 2nd moment estimates. Defaults to `0.999`.\n","\n","    epsilon = 1e-8\n","    # A small constant for numerical stability. This epsilon is \"epsilon hat\" in\n","    # the Kingma and Ba paper (in the formula just before Section 2.1),\n","    # not the epsilon in Algorithm 1 of the paper. Defaults to `1e-7`.\n","\n","    iteration_limit = 5000\n","\n","    # Moment estimates\n","    m = np.zeros_like(x) # Initialize 1st moment vector\n","    v = np.zeros_like(x) # Initialize 2nd moment vector\n","    iter = 0 # Iteration counter\n","\n","    # Initialize a variable to store the previous value of the objective function\n","    prev_loss = objective_function(x)\n","\n","    # Perform optimization\n","    while True:\n","        iter += 1\n","        # Compute the gradient of the loss with respect to x\n","        grads = adam_gradient(x)\n","\n","        # Update biased first moment estimate\n","        m = beta_1 * m + (1 - beta_1) * grads\n","        # Update biased second raw moment estimate\n","        v = beta_2 * v + (1 - beta_2) * (grads ** 2)\n","\n","        # Compute bias-corrected first moment estimate\n","        m_hat = m / (1 - beta_1 ** iter)\n","        # Compute bias-corrected second raw moment estimate\n","        v_hat = v / (1 - beta_2 ** iter)\n","\n","        # Update the variable x\n","        x -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n","\n","        # Compute current loss\n","        loss = objective_function(x)\n","\n","        loss_change = abs(prev_loss - loss)\n","        if loss_change < tolerance or iter >= iteration_limit:\n","            break # Stop the optimization when tolerance is met or max iterations reached\n","\n","        # Update the previous loss\n","        prev_loss = loss\n","\n","    return x, iter"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"elapsed":607,"status":"error","timestamp":1729192268229,"user":{"displayName":"Антон","userId":"12367554657538335492"},"user_tz":-180},"id":"D2g102dbof6-","outputId":"8ef75689-5bd2-486d-faee-ffb2896d6b8e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal x: [6.9996924  1.99966012]\n","Function value: 2.101366960765992e-07\n","Iterations: 165\n","Time taken: 1.5965 seconds\n"]}],"source":["x0 = np.array([0.0, 0.0])  # Initial guess\n","tol = 1e-6  # Desired tolerance\n","\n","start_time = time.time()\n","x, iter = adam_solver_numerical(x0, tol)\n","end_time = time.time()\n","\n","# Define the solution as a class with final results\n","class solution:\n","    def __init__(self):\n","        self.x = x\n","        self.func = objective_function(x)\n","        self.iters = iter\n","        self.time = end_time - start_time\n","\n","result = solution()\n","\n","print(f\"Optimal x: {result.x}\")\n","print(f\"Function value: {result.func}\")\n","print(f\"Iterations: {result.iters}\")\n","print(f\"Time taken: {round(result.time,4)} seconds\")"]}],"metadata":{"colab":{"collapsed_sections":["iWZQv0zXRu6O"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
